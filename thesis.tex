% This is the Reed College LaTeX thesis template. Most of the work 
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a 
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment. 
% They won't show up in the document, and are useful for notes 
% to yourself and explaining commands. 
% Commenting also removes a line from the document; 
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in 
% the 2002-2003 Senior Handbook. Ask a librarian to check the 
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\input{cryptomacros}
\usepackage{graphicx,latexsym} 
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace} 
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
\urlstyle{same}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{changepage}
\usepackage[vlined]{algorithm2e}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\graphicspath{./diagrams/}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

\title{Simulating Auctions}
\author{Robert S. Irvin}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2020}
\division{History and Social Sciences}
\advisor{Jeffery Parker}
%If you have two advisors for some reason, you can use the following
\altadvisor{David Perkinson}
%%% Remember to use the correct department!
\department{Economics and Mathematics FIXME}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

\setlength{\parskip}{0pt}
%%
%% End Preamble
%%
%% The fun begins:
\begin{document}

  \maketitle
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
    \chapter*{Acknowledgments}
	The cat, Pinoe.

% The preface is optional
% To remove it, comment it out or delete it.
    \chapter*{Preface}
	"For if we could suppose a great multitude of men to consent in the observation of justice and other laws of nature without a common power to keep them in awe, we might as well suppose all mankind to do the same; and then there neither would be, nor need to be, any civil government or commonwealth at all, because there would be peace without subjection." -Thomas Hobbes (Leviathan, chapter XVII)
	
	

    \chapter*{List of Abbreviations}
		You can always change the way your abbreviations are formatted. Play around with it yourself, use tables, or come to CUS if you'd like to change the way it looks. You can also completely remove this chapter if you have no need for a list of abbreviations. Here is an example of what this could look like:

	\begin{table}[h]
	\centering % You could remove this to move table to the left
	\begin{tabular}{ll}
		\textbf{AI}  	&  Artificial Intelligence\\
		\textbf{MAS}  	&  Multi-Agent System\\
		\textbf{MDP}    &  Markov Decision Processes\\
		\textbf{ML}     &  Machine Learning\\
	\end{tabular}
	\end{table}
	

    \tableofcontents
% if you want a list of tables, optional
    \listoftables
% if you want a list of figures, also optional
    \listoffigures

% The abstract is not required if you're writing a creative thesis (but aren't they all?)
% If your abstract is longer than a page, there may be a formatting issue.
    \chapter*{Abstract}
	The preface pretty much says it all.
	
	\chapter*{Dedication}
	You can have a dedication here if you wish.

  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

%The \introduction command is provided as a convenience.
%if you want special chapter formatting, you'll probably want to avoid using it altogether

    \chapter*{Introduction}
         \addcontentsline{toc}{chapter}{Introduction}
	\chaptermark{Introduction}
	\markboth{Introduction}{Introduction}
	% The three lines above are to make sure that the headers are right, that the intro gets included in the table of contents, and that it doesn't get numbered 1 so that chapter one is 1.

% Double spacing: if you want to double space, or one and a half 
% space, uncomment one of the following lines. You can go back to 
% single spacing with the \singlespacing command.
% \onehalfspacing
\doublespacing
	
	This thesis will lay out the literature on the price of anarchy bounds for single item, first price auctions and then will construct a simulation to demonstrate these bounds for no-regret agents. Chapter one hopes to be a broad overview of the field to give context for this discussion covering price of anarchy, auctions, and simulation. Chapter two will go through the mathematical theory in a more rigorous way laying out what sort of behavior we expect and why for no-regret agents in these auctions as well as how the price of anarchy for first price, single payer auctions were derived. Chapter three will be the empirical results of a simulation which hopefully accord with what the theory says. 
	
	\textbf{Note to the readers of this draft of chapter 1}: the in-text citations generated by Latex are formatted incorrectly/in the wrong style for this draft and I will talk to CUS to try and get this fixed. These will look like (Author1 \& Author2 (year)) which is incorrect.

\chapter{Computational Economics and Auctions}
	Within the last three decades, computational economics has been on the rise. This branch of economic research encompasses two major ideas. One is that the increasing power of computers can help solve and understand classical economic problems through increasingly more complex simulations and numerical analysis. Two, that the mathematical methods developed in the field of theoretical computer science can be used to gain better understanding of existing models in terms of their algorithmic and computational complexity properties. We aim to take elements from both of these frameworks to better understand the social welfare of auctions at equilibrium. 

\section{Auctions, Equilibria, and Anarchy}
It is perhaps obvious why economists would be inserted in studying auctions. Auctions are one of the most basic market structures that have roots going back to at least the ancient Greeks and still exist today in places such as art auctions and Ebay (\cite{Mochon2015}). But in the past 15 years, economists have been joined by computer scientists who are increasingly interested in the strategic interactions of agents within this setting. Christos Papadimitriou said in a 2015 lecture at the Simons Institute that it was the advent of the internet, an artifact out of their control, that turned theoretical computer science into a "physical science." Now computer scientists had to "approach the internet with the same humility that economists approach the market..." He went on to say that "it also turned us [computer science] into a social science. It was obviously about people and incentives. Without understanding this, you cannot understand the internet" (\cite{Papadimitriou2015}). It is within this framework that computer scientists first began to study auctions as they existed on the internet. At first, this meant trying to understand the auctions present there such as Ebay and Googles sponsored search auctions. But now, they are moving beyond that and taking the mathematical tools of theoretical computer science and applying them as a lens to understand and explain the world beyond the scope of the internet. Here specifically we are looking at the field algorithmic game theory, the study of the algorithms and complexity of strategic interactions. For economists, this can be thought of as a new toolbox for unpacking and understanding the models and structures that already dominate the field. For example, in the case of a Walrasian auctioneer who calculates the clearing prices of a combinatorial auction, their problem was shown to be NP-complete, a complexity class usually called "intractable" due to the time it takes to solve these problems (the best algorithms here are generally guess and check, which gets out of hand for large inputs i.e. possible combinations) (\cite{Papadimitriou2015})\footnote{NP is the class of problems that a given solution can be checked in polynomial time, i.e. $O(n^k)$ operations where $n$ is the size of the input and $k$ is any positive integer. NP-complete means that all problems in NP reduce to solving this problem. This is when you will usually hear "taking more time to compute then the age of the universe" etc...}. Using the lens of computational complexity is one way of assessing what assumptions we are making about the computational power of our rational agents.

\subsection{The Price of Anarchy}
Another idea that has come out of algorithmic game theory is the price of anarchy (POA), a mathematical way of showing the difference between the social welfare in the optimal case and in the worst case equilibrium for a game. More formally, the price of anarchy for a game is the ratio of the minimum equilibrium social welfare in a game over the best possible social welfare of the game. An illustrative example of the price of anarchy can be seen in selfish routing games such as the one pictured below in figure \ref{braess} that comes from Tim Roughgarden's 2016 book "20 Lectures on Algorithmic Game Theory". 
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{braess}
	\caption{Routing game from s to t. Strategic interaction will make everyone worse off.}
	\label{braess}
\end{figure}
In this game, each player chooses which edges to take on the directed graph from the source $s$ to reach the terminal $t$ and try to do it in the least time, total weight of edges taken, as possible. For the edges labeled x, x is equal to the proportion of players who take the route.\footnote{These figures are slightly off. The edges should have weights $f(x) = x$ or $f(x) = 1$ giving us the cost of using the edge as a function of the proportion of people who are using that edge. This will be fixed when I draw these graphs properly for the final draft and not in MS paint.} For example if $50 \%$ of the players take that edge then $x=0.5$. This can be seen as analogous to traffic when driving a car, the more people take a road, the slower the traffic goes and the longer it takes to get somewhere. Knowing this, each player must choose which path to take to get to their destination as quickly as possible. As you can quickly verify, the best solution for society, minimizing the total driving time for all, is when half of the drivers take the top route, and half of the drivers take the bottom route taking in total 1.5 for each driver. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{braess_2}
	\caption{Socially optimal route: half taking top, half taking bottom}
	\label{braess2}
\end{figure}

However, this is not an equilibrium. The drivers taking the top route have incentive to instead take the middle edge to try and lower their total time traveled. In fact, the Nash equilibrium will end up with all of the drivers taking the top x, going through the middle, and then the bottom x. Only then will no players have reason to deviate. These actions to individually and strategically try to decrease their cost end up producing a worse outcome for all players and society as a whole (the social welfare)\footnote{A careful observer will note that if the edge with cost 0 were not there, this would not be a problem. This is called Braess's Paradox where having this extra edge counter intuitively leads to worse outcomes. (\cite{Roughgarden2016})}.  

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{braess_3}
	\caption{Route under strategic interaction}
	\label{braess3}
\end{figure}

Now, it will take time 2 for each player to reach the terminal. Thus, under strategic interaction we see that the equilibrium is sub-optimal for society, and in this case all players as well. In this case, the price of anarchy can be computed as $$POA = 3/4$$. Interestingly, bounds for the price of anarchy can be found for the class of game as a whole regardless of its individual construction. The price of anarchy bound for these selfish routing games is know to be $3/4$, meaning that our example is as bad as the price of anarchy could get for any selfish routing game.

\subsection{Auctions}
This framework of the price of anarchy is now being applied to understand auctions by researchers in the algorithmic game theory field. Where for selfish routing games, the price of anarchy has been shown to have the tight bound above, the price of anarchy for single price auctions only has approximate upper and lower bounds set on it, and is not known at all for double auctions. Before exploring this further, we should set up the mathematical framework of auctions that is necessary to compute such bounds. 

An \textbf{auction} is a market mechanism, operating under specific rules that determines to whom one or more items will be awarded and at what price. For a bidder in an auction, the \textbf{value} $v_i$ is how much the bidder values the item. This is sometimes called a private valuation as this value is generally unknown by the other participants in the auction. The \textbf{bid}, $b_i$, is the offer that bidder $i$ submits for for an item. \textbf{Sincere bidding} is when $v_i = b_i$, \textbf{underbidding} is when $v_i > b_i$, and \textbf{overbidding} is when $v_i < b_i$. The highest bid made by any bidder is denoted $b^*$, and is the winning bid (if multiple bids equal the winning bid, then some tie-breaking rule must be used). The \textbf{selling price}, $p^*$ is the final price that the bidder actually pays for the item (which depending on the auction type need not equal $b^*$). Under the \textbf{first-price} rule, the bid submitted by the winner is equal to the selling price. Before the auction begins, each bidder knows their personal or private value for the item. An auction consists of a set of bidders, $I = (1,2, ...,N)$ and a seller. After the auction, the bidder $i$ wins the item if their bid is higher than the bid placed by any other bidder $k$, $b_i > max_{k \neq b_k}$. In a single unit auction, the \textbf{income of the bidder} $i$ is equal to their value of the item: $$ \Gamma_i^* = v_i$$ and the \textbf{surplus of the bidder} $i$ is equal to the difference between income and price paid $$ \Pi_i^* = \Gamma_i^* - p^*.$$ If a bid placed by a bidder is less than the winning bid, they do not win anything and their income and surplus are both zero. The \textbf{seller's revenue} in a single unit auction is equal to the price paid by the winning bidder $$ R^* = p^*.$$ 

There are multiple different pricing rules in an auction that determine who gets allocated the item. In a \textbf{First-Price} auction the winning bidder pays the amount of their bid, which is the highest bid of the auction: $p^* = b^*$. Also called \textbf{pay-what-you-bid} (PWYB). In a \textbf{Second-price} auction, the winning bidder pays an amount that is equal to the second highest bid for the awarded item \cite{Vickrey1961} \footnote{The second price, sealed bid auction is also known as a Vickrey auction after the economist who invented it}. People tend to bid lower than their private valuation in first price auctions since if they bid that value, their profit is zero. Thus, the first price rule the item could be awarded to someone who values the item less than other bidders. Second price auctions encourages bidder to bid their true values (as they will gain positive profits if they win no matter the second highest bid). This encourages an efficient allocation of items (\cite{Mochon2015}). For the moment we will confine ourselves to first price auctions as this is where most of the strong results in POA analysis of auctions currently are.

Before discussing the known bounds on the price of anarchy for first price, single payer auctions, it is worth understanding how the bids might lead to a non-optimal outcome or what we mean by that. Due to only knowing their own valuation of the good, each bidder must act under uncertainty as to how much they should bid to beat out the unknown valuations of the other bidders. However, if they bid their exact private valuation then they will get a surplus of zero, or zero utility. In order for them to get some utility for the item they must be paying less then the exact amount they value the item. How much each bidder should bid less than their valuation, or "shade", their bid is determined by how much they think that the other party values the item. To capture this interaction auctions are represented as Bayesian games where each bidder is drawing their bids from distributions known to the other player (that need not be the same). If one player knows that the other is drawing from a distribution with a smaller mean than they are (i.e. probably doesn't value the item as much), the Bayes-Nash equilibrium will have them shade their bid less and this other person will shade their bid more. This can lead to the person who values the item less winning the auction and creating less social welfare (the summed surplus of all bidders and the seller). 

Syrgkanis and Tardos proved in 2013 that the price of anarchy in first price, single item auctions is at least $1 - \frac{1}{e} \approx 0.63$. The exact upper bound on the price of anarchy for single payer, first price auctions remains unknown in the general case. This bound is true regardless of how many bidders there are or what distributions they are drawing their bids from.

This moves closer to answering the question for what is the price of anarchy at equilibrium, but these results do not pay attention to how the players arrive at these equilibria. In real world applications, we expect that players might play repeatedly in the same auction and learn as they play rather than come in with pre-computed strategies. This is especially true for when computing the equilibrium is computationally hard and the stakes of each individual auction is small. Given these observations, it is natural to ask questions about how the efficiency results carry over to adaptive game environments. The model for learning agents that is commonly used in the field is \textbf{no-regret learning}. An algorithm for a player satisfies the no-regret condition if, in the limit as the number of times the game is played goes to infinity, the average reward of the algorithm is at least as good as the average reward for the best fixed action in hindsight (assuming the sequence of actions for the other players remains unchanged)\footnote{That is to say that the algorithm will converge to having a loss no worse than any fixed strategy we would have rather picked in hindsight as the limit goes to infinity. A more precise definition, example algorithms, and uses will be shown in chapter 2 to clarify what "regret" is and how this converges to zero}. If each player  incorporates this kind of learning algorithm, then it has been shown that these can converge to a larger class of equilibrium called correlated equilibrium where each player conditions their response on the expected action of the other player. Luckily, the previous theorem has been extended so that we know that the price of anarchy for the correlated equilibria of first price auctions are also at least 0.63 (\cite{Roughgarden2017})

With all of this set up, we now state our goal: to simulate no-regret learning algorithms for agents in a first price, single payer auction to see how well the price of anarchy holds. We also want to compare this with other learning algorithms that aren't no regret to see if this framework of using these algorithms is appropriate for making generalizations about equilibrium under learning.

	
\section{Simulated Agents and Simulated Economies}
The use of computers in economics goes back all of the way to general purpose computers being invented in the 1940's. Wassily Leontif used a computer to invert a 39 x 39 matrix to help solve his input output model. Since then, computers use in economics has exploded. With computers, economists are able to solve bigger matrices, do Monte-Carlo simulations, create multinomial probit models, and use full information maximum likelihood estimation (\cite{Backhouse2016}). While one branch of computational economics is focused on creating stronger and stronger calculators to facilitate empirical research, another branch has focused on creating simulated economies that allow economists to construct a blended version of theory and research within a computer program. Within these simulated economies, theories can be coded into the simulation which, when run, can allow the researcher to conduct experiments that might not be practical to conduct in the real world. 

One kind of a simulation that can be run is called an agent-based model (ABM), a simulated system of autonomous decision makers (agents). These models are able to generate complex behavior even if only simple assumptions are made about the behavior of the coded agents. That is, these agents interacting with each other in complex ways are able to produce emergent phenomena in the macro structure of the system.

For example, in the 1970's Thomas Schilling created a computer simulation to try and understand how and why self segregated neighborhoods formed. He coded a virtual environment where agents were given a simple preference, they are only happy if they are not the minority in their neighborhood and will keep moving otherwise. This simple model can illustrate the main tenets of agent based modeling. First, we have agents who are representative of people in the real world. Their preferences to this respect are simple and easy to understand where they are only "happy" if half of their closest neighbors are the same as them. If they are not happy they will move somewhere else arbitrarily. These preferences can be represented as the short procedure, or algorithm, shown below where $S$ is just the space they live in.

\begin{adjustwidth}{1cm}{}
	\textbf{1.} Draw a random location in $S$ \\
	\textbf{2.} If happy at new location, move there\\
	\textbf{3.} Else, go to step 1
\end{adjustwidth}

In this case, we get to choose what that environment looks like and like Shelling we can just say that it is a one by one unit square and we can say that their neighbors are the ten closest people to them (in Euclidean distance) on that square. When you run this simulation with green and orange dots representing the types of people you get the following behavior cycling through each of our 250 agents with the above procedure until every agent is happy.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{segregation_1}
	\caption{Schelling's Segregation Model: Cycle 1}
	\label{SSM1_ch1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{segregation_2}
	\caption{Schelling's Segregation Model: Cycle 2}
	\label{SSM2_ch1}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{segregation_3}
	\caption{Schelling's Segregation Model: Cycle 3}
	\label{SSM3_ch1}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{segregation_4}
	\caption{Schelling's Segregation Model: Cycle 4}
	\label{SSM4_ch1}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{segregation_5}
	\caption{Schelling's Segregation Model: Cycle 5}
	\label{SSM5}
\end{figure}

As can be seen, in cycle 1 (Figure \ref{SSM1_ch1}), the agents are well distributed among each other, but as they move in cycles 2-5, they become progressively more segregated. After 5 cycles all agents are happy and the simulation terminates. With very few assumptions about the agents' preferences, we can see the resulting emergent behavior of segregated neighborhoods in the system as a whole. Not only that, but the agents naturally move to an equilibrium as they adjust their behavior to what their neighbors are doing (\cite{Sargent2019}). 

Agent based modeling has been used to build and understand much more complicated systems then the example illustrated above. The Santa Fe Institute in New Mexico is one of the main proponents of agent based modeling releasing a manifesto supporting using it to understand the "complexity" of economics from the ground up (\citealp{Backhouse2016}). They built the Santa Fe Artificial Stock Market in the 1990's to try and simulate the behavior of agents on the stock market and how they adapt their adapting trading strategies effects the outcome of the market. This is one of the first examples of agents learning and adapting to their environments as part of the model (\cite{LeBaron2002}). In this model agents used a genetic algorithm to adapt their trading strategy at each period by modifying a string (for example $00011100$) where each bit in the string told the agent to use a certain behavior or not. Those trading strategies that did well were coded to survive longer where the agents with worse strategies would randomly modify their own or take a more successful agents strategy (\cite{Arthur1992}). This was supposed to be representative of the learning of traders on the stock market so that the insights taken from running these agents in simulation could be applied to learn something about the real world. 

The authors of Santa Fe Stock Market paper at the time suggested that this was one of many algorithms that could be used to stand in for human behavior saying that reinforcement learning or deep learning could also be used to stand in for human intelligence\footnote{Reinforcement learning is when an algorithm plays a game repeatedly and updates its beliefs about what actions will lead to the best payoff. Deep learning uses deep neural networks to try and estimate the best outcomes in a fashion similar to regression.} (CITATION NEEDED, This was in Shareen Joshi's thesis and in one of their papers, but I am having trouble tracking down the quote. Will use her thesis to find original source or remove reference). Recent research in the field as well as in the field of multi-agent systems, a similar branch of computer science suggest that this is not the case. Because these simulations have agents competing in non-stationary environments that are changing from the perspective of any individual agent every time some other agent changes their behavior, the choice of algorithm dramatically changes how the system behaves (\cite{Rejeb2005}, \cite{Shoham2008}). Further, research that has compared the behavior of human agents in strategic settings to that of these algorithms have found that there is no general algorithm that best approximates human learning(\cite{Tesfatsion2002}). From situation to situation, different algorithms more appropriately behave like humans. This is a problem only assuming that you want your agents to in some way represent human behavior, we might simply want our agents to represent the "rational choice" in any given situation. These algorithms aren't necessarily doing that either. As Holland and Miller say in their 1991 paper "Artificial Adaptive Agents in Economic Theory", "Usually there is only one way to be fully rational, but there are many ways to be less rational." The way they suggest to get around this is to try and build models that have robust behavior across algorithm choice. This is probably not true for their own artificial stock market as it is not even robust across the choice of parameters to program the market. The literature of game theory provides a nice solution to this. The no-regret algorithms they use have the simple property of doing better in the long run than fixing their strategy randomly in the beginning. This is a simple learning requirement that means that this kind of algorithm should be more robust across representing human behavior. It also allows us to combine our simulated models and analytical models in a nice way as we can mathematically describe the processes of such algorithms behavior and we can code agents to use algorithms that have that behavior. This allows the learning process to be less of a black box and instead to be as simple and thus generalizable as possible.

Moving forward this thesis aims to better understand the price of anarchy in auctions by simulating auctions using these no-regret learning algorithms\footnote{these will be discussed in chapter 2, but no-regret learning is a type of reinforcement learning that have certain convergence properties} and seeing how they behave compared to the socially optimal equilibrium. These simulations will try to answer two main questions, what equilibrium do these algorithms converge to (if they converge at all) and what is the calculated price of anarchy in these systems compared to the bounds that theory tells us should exist. Using this framework to explore auctions we will start with a single-item first price auction as it is the best understood theoretically and is the easiest to code. We will then move to simultaneous first price auctions for complementary goods as this is also relatively well understood in terms of the price of anarchy. Finally, time permitting, we will discuss the open question of the price of anarchy in double auctions and discuss how one might go about proving bounds for it.

\chapter{Price of Anarchy in First-Price Auctions}
This chapter lays out the mathematical framework for modeling auctions as Bayesian games of incomplete information, formally defines the price of anarchy in auctions and shows the bounds on the price of anarchy for first price, single payer auctions. It then follows up on this framework of analysis to show how these bounds can be extended to agents competing in auctions using learning algorithm's with a property called "no-regret" learning. This chapter primarily follows from the work of Tim Roughgarden, Vasilis Syrgkanis, and \'Eva Tardos who not only are all individually active in the field of algorithmic game theory and auctions, but also jointly authored the 2017 paper "The Price of Anarchy in Auctions" that is a survey of the entire topic (\cite{Roughgarden2017}). This chapter will be giving the important theorems and ideas that primarily come from this work and the individual work of these three authors, explaining them, demonstrating proofs where appropriate (I need to talk about where proof is appropriate with both advisors after you read this. I did not prove anything and am just citing/reconstructing results), and reconstruct the results that we will demonstrate with a simulation.   

\section{Auctions as Bayesian Game}
Auctions are typically modeled as Bayesian games also known as games with incomplete information. While it is obvious why an auction consisting of the strategic interaction of bidders could be modeled as a game, how it should be modeled requires some thought. After all, while each bidder knows their own valuation of the item being sold, unless for some reason (and against their own interest) the other players announced their valuation of the item before the auction began our bidder will not know how the other players value the item. In fact, it is precisely this lack of information in sealed bid auctions that makes them interesting! If all players came into an auction knowing the valuation of other players, for first-price, single-item auctions assuming there was no tie, the bidder with the highest value would always win by bidding the valuation of the next highest bidder. Rather it is the uncertainty that players face about the valuation of other players that make things interesting as players must guess how much they should shade their bid (as again no player will ever bid above their valuation) based off of what they know about other players. 

What can we say that bidders know about the valuations of other bidders? Certainly they do not know nothing as we all have reasonable expectations about what some item is worth to others. No one will value a candy bar at a million dollars. But a collector of candy bars might value it at a higher value than an ordinary person who just want to eat the candy bar. For each bidder, we could say that this bidder has a probability distribution from which they are drawing their valuation from that is know to the other bidders. As an example we might expect normal people's valuations of candy bars to be a normal distribution centered at \$1, but the collectors value might be a leplacian distribution (allowing for more black swan events) centered at \$5. This sort of strategic interaction where the players know the distributions that parameters of the game are drawn from are typically called games of incomplete information, or Bayesian games.
 
\subsection{Bayesian Games}
Bayesian games of incomplete information are games in which one or more of the players don't have "full knowledge" of the game that is being played. Introduced by John C. Harsanyi in 1967, rather than players knowing every parameter of the game situation such as utility functions, possible strategies, and information held by other players, each player knows a probability distribution from which these will be drawn. In his paper, Harsanyi says that this type of game can be thought of as a normal game, where "nature" goes first drawing from these probability distributions and assigning values before play begins without the players knowing which specific variation of the game they are playing (TODO CITE Harsanyi). Importantly, each player does know the probability distributions from which each value is selected. Formally such games are defined as follows, 

\begin{dfn}
	A game with incomplete information, also known as a Bayesian game where there are $n$ players, $G = (\F, S, P, u)$ %(TODO check if P or p)% 
	consists of:
	\begin{enumerate}
		\item A set $\F = \F_1 \times \ldots \times \F_n$, where $\F_i$ is the finite set of possible types for player $i$.
		\item A set $S = S_1 \times \ldots \times S_n$, where $S_i$ is the set of possible strategies for player $i$
		\item A joint probability distribution $p(\F_1, \dots , \F_i)$ over types. For finite type space, assume that $p(\F_i) > 0$ for all $\F_i \in \F$
		\item Payoff functions $u_i : S \times \F \rightarrow \mathbb{R}$
	\end{enumerate} 
\end{dfn} (TODO CITE Levin)

Using this definition to model our auction the types of players will consist of the publicly known distribution from which they are drawing their valuation. That is, our auction will consist of bidders who know their own valuation of the item being bidded on, and the distribution from which each of the other players is drawing their own valuations. In a first-price, single item auction if player $i$ wins with bid $b_i$, we define their payoff the winner to be $u_i = v_i - b_i$, the difference between their valuation and their bid and the losers all get $u_i = 0$ since they did not receive the item. Now, a strategy for a player is a function $s_i \in S_i$ that maps a valuation $v_i$ in support of $\F$ to a bid $s_i(v_i)$ (i.e. taking into account the probability distribution for the other players). 

Now, we move to the idea of equilibrium in this system. In games of complete information the central equilibrium concept is usually a Nash equilibrium, the set of strategies for all players in which each individual player cannot increase their utility by deviating from their strategy fixing the strategy of all the other players. That is, for every player if no one else changes strategy, their best option is to stay where they are, hence an equilibrium. This concept is now updated to give us a Bayes-Nash equilibrium where we must also incorporate the distributions for which players are drawing from. 

\begin{dfn}
	A strategy profile constitutes a Bayes-Nash equilibrium if for every player $i$ and every valuation $v_i$ that the player might have, the player chooses a bid $s_i(v_i)$ that maximizes her conditional expected utility where the expectation is over the valuations of the other players, conditioned on a bidder i's valuation being $v_i$. 
\end{dfn}
%TODO: confusing use of dfn environment

\subsection{Formally Defining First-Price Auctions}
NOTE: This section needs cleaning up since it is going to give a lot of duplicate information or redefine things slightly differently from previous sources.

To analyze an first-price auction as a game, we give the notation we will be using. This notation comes from Roughgarden et. al 's 2017 survey of the subject of the price of anarchy in auctions. For a bid profile $\textbf{b} = (b_1, \ldots, b_n)$, we let 
\[
	x_i(\textbf{b}) =
	\begin{cases}
		1 & \text{if player $i$ is the winner} \\
		0 & \text{otherwise}
	\end{cases}
\].

We let $p(\textbf{b}) = \max_{i \in \{1, \ldots, n\} } b_i$ denote the selling price. The utility that a player $i$ receives when their valuation is $v_i$ is 
$$u_i(\textbf{b}; v_i) = (v_i - b_i) \cdot x_i(\textbf{b})$$. 

Now, for a strategy profile $\textbf{s} = (s_1, \ldots, s_n)$, where each $s_i$ is a function for player $i$'s valuation $v_i$ to their bid. We let $\textbf{s}(\textbf{v})$ denote the strategy vector resulting from the vector of valuations $\textbf{v}$. For any given vector $\textbf{x}$, we use $\textbf{x}_{-i}$ to denote the vector $\textbf{x}$ with the $i$th element removed. 

Now, for a first-price auction we can say that a strategy profile $\textbf{s} = (s_1, \ldots s_n)$ is a Bayes-Nash equilibrium if and only if 
$$ \mathbb{E}_{\textbf{v}_{-i}} [u_i(\textbf{s} (\textbf{v}); v_i) \ | \ v_i] \geq \EXP_{\valuations_{-i}} [u_i(b^{'}_i, \strategies_{-i}(\valuations_{-i}); v_i) \ | \ v_i] $$.

(\cite{Roughgarden2017})

\subsection{Example Auction}
We now turn to an example auction to clarify what has just been laid out. In this example we analyze the an auction between two players, Alice and Bob who are bidding on a candy bar where each select their valuations from the uniform distribution $[0,1]$. This is a first-price, sealed bid auction where they each submit a bid for the candy bar simultaneously. How are Bob and Alice supposed to decide what to bid on this auction? 

\begin{prop}
	In the first-price, sealed bid auction with valuation distributed on $[0,1]$, the unique Bayesian-Nash equilibrium is $\strategies = (s_1(v_1) = v_1 / 2, s_2(v_2) = v_2 / 2)$.
\end{prop}

To show this first, we show that each player is using a best response. First, we note that We calculate that the expected value for player one is,

\begin{align*}
	\EXP_{\valuations_{-1}} [u_1(\strategies(\valuations); v_1 \ | \ v_1)] &= \EXP_{v_2}[u_1( \ s_1(v_1), s_2(v_2); \ v_1 \ | \ v_1)] \\
	&= (v_1 - b_1) \Pr[s_2(v_2) < b_1] + \frac{1}{2}(v_1 - b_1)\Pr[s_2(v_2) = b_1].
\end{align*}

since player expect to get their full utility with the probability that they outbid the other player, and expect to get one half that if they tie with the other bidder (since ties are broken with a coin toss). Now, assuming that player one bids, $b_1 \in [0, \frac{1}{2}]$.

... I'm not sure if I need this, leaving it out to write up more important parts.

\subsection{Efficiency of First-Price Auctions}
Examples of the Bayes-Nash equilibrium have been solved for various combinations of the number of players and distributions from which they draw their valuations. With $n$ bidders it has been shown that the Bayes-Nash equilibrium strategy vector is composed of $s_i(v_i) = \frac{n-1}{n} v$ for all players $i$ (TODO: cite, find source). Here the equilibrium is easy to calculate and efficient (meaning that the item will always be allocated to the player with the highest valuation). Neither efficiency nor ease of calculation are guaranteed for Bayes-Nash equilibria in this auction format. For example if we conduct an auction with two bidders, one choosing from the uniform distribution $[0,1]$ and the other from the uniform distribution $[0,2]$ it has been shown that the Bayes-Nash equilibrium for this auction is:
\begin{align*}
	&s_1(v_1) = \frac{4}{3 v_1} \left(1 - \sqrt{1 - \frac{3v_1^2}{4}}\right)\\
	&s_2(v_2) = \frac{4}{3 v_2} \left(\sqrt{1 + \frac{3v_2^2}{4}} - 1 \right)\\
\end{align*}
 
(CITE Krishna, 2002):

Here bidder one with the smaller valuation distribution knows that bidder two is more likely to to have a higher valuation than them. Thus player one must bid higher relative to their given valuation if they expect to win and so they shade their bid less than bidder two. This can lead to bidder one drawing a lower valuation than bidder two, but outbidding them regardless and winning the item. This is inefficient. More over, it has been shown that solving many of these asymmetric Bayes-Nash equilibrium requires finding a solution to a system of partial differential equations many of which have no closed-form solution (\cite{Roughgarden2017}). This means that if we expect bidders to do their homework before an auction and choosing their bidding strategy they might not know what to do. Given this, it is extremely hard to characterize or say things about what solutions to this format of auctions look like in general. However, just because we are not able to give a closed form for all of these equilibria, that does not mean we aren't able to say anything about them. 

\subsection{Price of Anarchy in First-Price Auctions}  
To try and get a sense of how inefficient these auctions can be, computer scientists have been applying a concept known as the price of anarchy to analyze these systems\footnote{Not that economists were uninterested in these questions of efficiency and welfare analysis in auctions before the computer scientists started studying them.}. The price of anarchy is a way to compare the social welfare of a system or a game at its best possible value to that of its worst possible equilibrium under strategic play. We must first define these concepts and then move to the point at hand. In the case of an auction the social welfare is the sum of the utilities of the players plus the revenue of the auctioneer.

\begin{dfn}
	The \textit{social welfare} of a bid profile $\bidders$ when the valuation profile is $\valuations = (v_1, \ldots, v_n)$ is 
	$$ SW(\bidders;\valuations) = \sum_{i=1}^{n} v_i \cdot x_i (\bidders)$$
\end{dfn}

The price the winning bidder pays does not appear in this equation since the winning bidder is paying exactly as much as the auctioneer is getting and this term cancels out. This means that welfare is maximized when the bidder with the highest valuation is winner. Thus, if we let $x^*_i(\valuations)$ be an indicator variable for whether or not a player $i$ is the player with the highest valuation (ties broken arbitrarily), the maximum possible social welfare in a single-item auction is 
$$ \OPT(\valuations) = \sum_{i=1}^{n} v_i \cdot x^*_i (\bidders)$$.

Now, that we mathematically describe the social welfare in our system, we can define the price of anarchy. 

\begin{dfn}
	The \textit{price of anarchy} of an auction, with a valuation distribution $\mathcal{F}$, is the smallest value of the ratio
	$$ \frac{\EXP_{\valuations} [SW(\strategies(\valuations);\valuations)]}{\EXP_{\valuations}[\OPT(\valuations)]},$$
	ranging over all Bayes-Nash equilibrium $\strategies$ of the auction.
\end{dfn}

The above definition applies to individual auctions dependent on choice  of $\F$ and $n$. We generally only discuss the price of anarchy for the format of the auction, in our case any possible first-price auction.  The price of anarchy for the first-price auction format is then the worst possible price of anarchy for any choice of the number of players $n$ or valuation distributions $\mathcal{F}$. Note that the price of anarchy (for either an individual auction or the format of auction) is a number between $0$ and $1$, and that the closer it is to one, the "better" we can guarantee the system's social welfare will be\footnote{Much of the literature for POA (including the paper introducing the idea) defines it as the opposite ratio, Optimal/Worst-EQ where smaller values indicate better systems. For some reason the auction literature defines it as Worst-EQ/Optimal, so I will remain consistent with them.}.

Incredibly, bounds on the price of anarchy for the format of first-price auctions have been found. Again, this allows us to characterize how much worse the social welfare for the system could be at (Bayes-Nash) equilibrium no matter how many players we have or what distributions they are choosing their valuations from. This sort of guarantee is incredible, especially for systems where we may not want, or it may not be feasible to have a central authority pre-calculate the way to optimize social welfare in a system. Rather, we can trust that the system will perform at least so well under strategic interaction.

\begin{thm}
	The price of anarchy in first-price single-item auctions format is at least $1-\frac{1}{e} \approx 0.63$	
\end{thm}

(CITE: Syrgkanis \& Tardos 2013)(Possibly include: no bound better than $0.87$ is possible (CITE Hartline and Taggart 2014))

(Possibly include below thm, might be referenced by later proofs)
\begin{thm}
	Every Bayes-Nash equilibrium of a first-price auction with correlated valuation distributions has expected social welfare at least $1 - \frac{1}{e}$ times the optimal welfare.
\end{thm}

(CITE: Syrgkanis, 2014) (Tight bound for above)

Theorem 1 tells us that no matter  how many players we have or what weird distributions we try and give them, we cannot construct a first-price auction that will achieve less than $0.63$\% of the optimal social welfare at Bayes-Nash equilibrium.
 
\section{Extending Results to No-Regret Agents}
These results hold for simple first-price auctions, but it is natural to ask questions about how robust these results are. First of all, how do people arrive at a Bayes-Nash equilibria if there doesn't exist a closed form way to express it? Secondly, do these results hold for mixed Bayes-Nash equilibria (randomizing between bidding strategies) or other larger, more realistic equilibrium concepts? To address these questions, Roughgarden et al use a set of extension theorems that take us through a general mechanism design setting and allow the class of equilibria our bounds hold for to be expanded. This extension will take us to a concept of no-regret learning.

\subsection{General Auction Mechanisms}

In order to understand (or even state) the proofs and theorems required to take us to our expanded set of equilibria, we must introduce  more notation and the idea of a general mechanism design setting and a general auction setting (this is unfortunate for us due to an expansion of scope, but actually makes these theorems quite powerful!). In a general mechanism design setting, the auctioneer solicits an action $a_i$ from all of the players, $i$, from some action space $\mathcal{A} = \mathcal{A}_1 \times \cdots \times \mathcal{A}_n$. Given an action profile $\textbf{a} = (a_1, \dots, a_n) \in \mathcal{A}$, the auctioneer decides an outcome $o(\textbf{a})$ among the set of possible outcomes $\mathcal{O}$. This outcome includes a payment $p_i(o)$ that each player must give to the auctioneer. Denote the revenue of the auctioneer $\mathcal{R}(O) = \sum_i p_i(o)$. Players receive some utility as a function of their valuation, $v_i$ and the outcome which we write $u_i(o;v_i)$. Let $\mathcal{V} = \mathcal{V}_1 \times \cdots \times \mathcal{V}_n$. Now with this new notation out of the way, we move to the concept of a smooth auction.

\subsection{Smooth Auctions}

\begin{dfn}
	For parameters $\lambda \geq 0$ and $\mu \geq 1$, an auction is $(\lambda, \mu)$-smooth if for every valuation profile $\valuations \in \mathcal{V}$ there exist action distribution $D_1^{*}(\valuations), \ldots , D_n^*(\valuations)$ over $\mathcal{A}_1, \cdots, \mathcal{A}_n$ such that for every action profile \textbf{a},
	$$ \sum_i \EXP_{a_i^* \sim D_i^*(\valuations)}[u_i(a^*, \textbf{a}_{-i};v_i)] \geq \lambda \OPT(\valuations) - \mu \mathcal{R}(\textbf{a})$$
\end{dfn}

\begin{thm}
	If an auction is $(\lambda, \mu)$-smooth, then for every profile $\mathcal{F}_1, \ldots, \mathcal{F}_n$ of independent valuation distributions over $\mathcal{V}_1, \ldots, \mathcal{V}_n$, every Bayes-Nash equilibrium of the auction has expected welfare at least $\frac{\lambda}{\mu} \cdot \EXP_{\valuations \sim \mathcal{F}}[\OPT(\valuations)]$.
\end{thm}  
(\cite{Roughgarden2017})

\subsection{No-Regret Learning}
Consider an auction with $n$ players that is repeated for $T$ time steps. At each iteration $t$, bidder $i$ draws a valuation $v_i$ from $\mathcal{F}_i$ and chooses an action $a_i^t$ which can depend on the history of play. After each iteration, the players observes the actions taken by the other players (TODO: potentially can be relaxed to only need to see utility for their own action, which makes simulating easier. Need to follow up on source). A player $i$ is said to use a no-regret learning algorithm if, in hindsight their average regret (difference between average utility of strategy vs algorithm) for any alternative strategy $a_i^{'}$ goes to zero or becomes negative as $T \rightarrow \infty$. When all players use this algorithm it results in a vanishing regret sequence.

\begin{dfn}
	A sequence of action profiles $\actions^1 , \actions^2, \ldots, a^T$ is a \textit{vanishing regret sequence} if for every player $i$ and action $a_i^{'} \in \mathcal{A}_i$,
	$$ \lim\limits_{T \rightarrow \infty} \frac{1}{T} \sum_{t=1}^{T}(u_i(a_i^{'}, \actions_{-i}^{t};v_i) - u_i(\actions^t; v_i)) \leq 0$$ 
\end{dfn}

\begin{thm}
	If an auction is ($\lambda, \mu$)-smooth, then for every valuation profile $\valuations$, every vanishing regret sequence of the auction has expected welfare at least $\frac{\lambda}{\mu} \cdot \OPT(\valuations)$ as $T \rightarrow \infty$.  
\end{thm}

(\cite{Roughgarden2017}) (TODO: add second source)\\

The end result is that since single-payer first-price auctions are $(1-\frac{1}{e}, 1)$ smooth (TODO, demonstrate that), we know that this bound will hold for auctions with no regret learning agents. Thus, we will try and build a simulation that uses one of the algorithms that fulfills this property and demonstrate that for any arbitrary number of players and distributions from which they pick their valuations from, they converge to an equilibrium social welfare greater than $1-\frac{1}{e}$.

\chapter{Simulating the Price of Anarchy}

This chapter constructs a novel simulation of first-price single-payer auctions to demonstrate that the ratio of actual social welfare to optimal social welfare is within the bounds of the price of anarchy. First, construction of the simulation is discussed. Next, we demonstrate the results of running the simulation on bidders using known Bayes-Nash equilibrium strategies and arbitrary bidders. Finally, we demonstrate that these bounds also hold for no-regret learning agents in a fixed action variation of the first-price auction.

\section{A Simulated Auction Environment}

To simulate sequential first-price auctions, we construct a program in python that allows us to set up an arbitrary number of bidders each with valuation distributions of our choice who simultaneously bid on the item being auctioned that round for as many rounds as we choose. Since these are sequential auctions, each round represents an auction for a new item; however the valuation distributions remain the same for each auction and the utility is cumulative across all auctions. Hence, at the beginning of each round, every bidder is assigned a valuation sampled from their distribution. The bidders then each simultaneously submit a bid to the auction and the winner is determined by the highest bid where ties are broken with equal probability among those with the same bid. After the winner is selected, they are given utility $u(v, b) = v - b$, the difference between their valuation and bid that round. At each round the total social welfare is $v_i$, the valuation of the winning bidder as per definition 3. (TODO add internal ref). The total social welfare generated across each sequential auction is tracked by sequentially adding up the social welfare for each round. Similarly we keep track of the total optimal social welfare by summing them across rounds. The optimal social welfare at any given round is the highest valuation held by any bidder that round. This is laid out in the pseudo-code version of the sequential auction below where we use the super script $t$ to denote which round each variable is from\footnote{The "$\leftarrow$" symbol used in the algorithm means assignment of value. For example $x \leftarrow 1$ is the variable $x$ is assigned a value of $1$. This reduces the ambiguity of using the "$=$" symbol which could be also be a statement or proposition in pseudo-code.}.\\

\begin{algorithm}[H]
	Initialize $SW$, $OW$, and $POA$ to zero\\
	\For{$t = 1, \ldots, T$}
	{
		Each bidder draws their valuation $v^t_i$ from their distribution $F_i$\;
		Each bidder uses their strategy $s^t_i$ to submit a bid\;
		The highest bidder is assgined the object and they pay their bid, if tie, a winner is choosen randomly among them\;
		Each player has their utilites updated according to if they won the object\;
		\If{player $i$ wins the auction}{
			$SW \leftarrow SW + v^t_i$
		}
		\If{player $j$ has the highest valuation}{
			$OW \leftarrow OW + v^t_j$
		}
	
		$POA \leftarrow \frac{SW}{OW}$	
	\caption{Sequential First-Price Single-Item Auction}
	}
\end{algorithm}


\subsection{Simulating Bayes-Nash Equilibria}
Using the simulation outlined above, we first demonstrate that bidders using known Bayes-Nash equilibrium strategies have an average price of anarchy greater than $0.66$. First, we simulate the case of two bidders each drawing their valuation from the uniform distribution between $0$ and $1$. We are using the hard coded strategies that $s(v^t_i) = \frac{v^t_i}{2}$ for each bidder which was shown to be the unique Bayes-Nash Equilibrium in chapter 2.The results are shown in table \ref{table:1} below where the cumulative price of anarchy is given up to the specified round specified \footnote{The POA is calculated as per algorithm 1. %(TODO Fix manual ref).
}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c| }
	\hline
	Round & POA \\
	\hline
	1 & 1.0000 \\
	10 & 1.0000 \\
	100 & 1.000 \\
	1,000 & 1.0000 \\
	10,000 & 1.0000 \\
	100,000 & 1.0000 \\
	\hline
\end{tabular}
\caption{Price of anarchy in two player symmetric auction}
\label{table:1}
\end{center} 
\end{table}

As can be seen this example is somewhat silly to simulate since this equilibrium is fully efficient. As each player bids half their valuation, the winner is always the player with the highest valuation. Hence the actual social welfare is always the same as the optimal social welfare: $$\frac{SW(\strategies(\valuations); \valuations)}{\OPT(\valuations)} = 1.$$

We now move to simulate the more interesting example of two bidders with asymmetric distributions. We let bidder one chose their valuation from the uniform distribution over the interval $[ 0,1]$ and bidder two chooses their valuation from from the uniform distribution on the interval $[0,2]$. As stated in chapter 2, the unique Bayes-Nash equilibrium for this auction is:

\begin{align*}
&s_1(v_1) = \frac{4}{3 v_1} \left(1 - \sqrt{1 - \frac{3v_1^2}{4}}\right)\\
&s_2(v_2) = \frac{4}{3 v_2} \left(\sqrt{1 + \frac{3v_2^2}{4}} - 1 \right)\\
\end{align*}

As stated in chapter 2, this auction is not fully efficient as the bidder who is drawing their valuation from the smaller distribution has to shade their bid less (bid higher relative to their valuation) in this equilibrium. We simulate this sequentially 100,000 times and get the following results as shown in table \ref{table:2}:
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c| }
			\hline
			Round & POA \\
			\hline
			1 & 1.0000 \\
			10 & 1.0000 \\
			1,000 & 0.9919 \\
			10,000 & 0.9924 \\
			100,000 & 0.9935 \\
			\hline
		\end{tabular}
		\caption{Price of anarchy in two player asymmetric auction}
		\label{table:2}
	\end{center} 
\end{table}

While this auction is not fully efficient, it is highly efficient. The price of anarchy in this auction never drops below 0.99. The main reason this happens is that while it is possible for the bidder drawing from the smaller distribution to win even if they have the smaller valuation, this only occurs when their two valuations are relatively close. This means that the social welfare lost in this case is not much even if it is not fully efficient. In both of these auction we see that they are well above the $0.66$ lower bound guaranteed for all first-price single-item auction formats.

\subsection{Minimal Intelligence Bidders}
Before going on to simulate the auction using no-regret learning bidders, it is important to establish some sort of baseline for how well each auction setting performs with agents that are not learning. To do this we construct agents that bid randomly between zero and their valuation every round i.e. each players bid is chosen uniformly from the distribution $[0,v^t]$. We call these agents minimally intelligent since they are not overbidding, but they are also clearly using a nonsensical strategy for an auction. One should note that while this is a bad strategy, it is possible to formulate much worse strategies for our bidders from both utility maximization and efficiency standpoints \footnote{It's rather fun to think up such strategies! For example, if each bidder shaded little when they had a low draw and shaded a lot when they had a high valuation, this can lead to highly inefficient outcomes}. This rather is a baseline for agents who are incapable of learning or doing their homework and thus randomly select from all options. Since this is a baseline we also want to see how this auction format behaves as we add more agents or bidders to each round. The results for simulating sequential first price auctions with 2, 10, and 100 agents using this strategy are shown in the table \ref{table:zero_int_symmetric} below.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c|c|c| }
			\hline
			Round & 2 Agent POA & 10 Agent POA & 100 Agent POA \\
			\hline
			1 & 0.9000 & 0.8410 & 0.9859\\
			10 & 0.9312 & 0.8987 & 0.9419\\
			1,000 & 0.9279 & 0.8812 & 0.9480\\
			10,000 & 0.9150 & 0.8880 & 0.9467\\
			100,000 & 0.9164 & 0.8887 & 0.9470\\
			\hline
		\end{tabular}
		\caption{Price of anarchy in two player asymmetric auction}
		\label{table:zero_int_symmetric}
	\end{center} 
\end{table}

This gives us something of a baseline for the efficiency of single-item sealed bid auctions when all of the players are symmetric and at these number of players. We see three things, one is that when all bidders are drawing from the same uniform distribution the market is incredibly efficient regardless of how smart the bidders are when choosing their strategy. That is, even when their strategies are arbitrary, this auction still performs reasonably well. The second thing to note is that in this setting, as the number of bidders increases the social welfare also increases. This makes sense as we would expect the probability of a reasonably high valuation winning to increase as there are  more valuations per round. The third thing we notice, and most surprising of all, is that the increase in the price of anarchy as the number of players increases is not monotonic. (TODO: Graph for 2-100 agents on IMC comp)

We now conduct a similar simulation but in the case of asymmetric bidders. In this case Half of the bidders are drawing uniformly from $[0,1]$ and half from $[0,2]$. The results are shown in table \ref{table:zero_int_asymmetric}. 

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c|c|c| }
			\hline
			Round & 2 Agent POA & 10 Agent POA & 100 Agent POA \\
			\hline
			1 & 0.1.000 & 1.0000 & 0.9726\\
			10 & 0.9115 & 0.8178 & 0.9331\\
			1,000 & 0.9054 & 0.8658 & 0.9310\\
			10,000 & 0.9092 & 0.8630 & 0.9297\\
			100,000 & 0.9112 & 0.8640 & 0.9304\\
			\hline
		\end{tabular}
		\caption{Price of anarchy in two player asymmetric auction}
		\label{table:zero_int_asymmetric}
	\end{center} 
\end{table}

Again we see that the market is quite efficient in this case regardless of how smart the bidders are. We again also see that it seems to converge to optimal (i.e. to 1) as the number of bidders increases but non-monotonically. (TODO: GRAPH for 2 to 100)

Note that the baseline price of anarchy for minimal-intelligence agent auctions changes as we change the distributions. While the format seems quite efficient and well above the price of anarchy bounds (which again are only guaranteed for equilibria which this is not) even with agents following these minimally intelligent strategies, we have no guarantee that there is not someway to construct this that these agents wouldn't do much worse. Given that it seems as the number of agents increases, the efficiency also seems to increase, one would expect that this would need to be done by picking more interesting distributions for the bidders to choose from and only having two bidders. (NOTE: I am trying to find scenarios that do just that)

\section{Simulating No-Regret Bidders}
With the results from the Bayes-Nash equilibrium demonstrated and a baseline established, we move on simulating the bounds of coarse correlated equilibria in first-price auctions. Again, this is the equilibria we expect auctions to converge to if each agent is using a no-regret learning algorithm. To use these algorithms we do however have to make a concession to the environment we are simulating: we must now simulate an auction where the bidders are only allowed a finite number of actions.

\subsection{Multiplicative Weights Algorithm}
The no-regret learning algorithm we will use to train our bidders is called the multiplicative weights algorithm. It has been shown by (TODO, add all sources) to satisfy the no-regret property and thus if each of our bidders use it our auction should converge to a coarse correlated equilibrium. 

Before giving the algorithm, a few words are probably necessary to understand where it comes from and what it is trying to do. First, this algorithm is what is called an \textit{online} algorithm. That is an algorithm that takes its inputs sequentially as it goes rather than getting all of its inputs up front. Next, this algorithm and many other algorithms for players learning in games are based around the player only having a fixed number of actions they can choose from. For each round in the game, the player gets outside advice from "experts" who recommend to the player what to do at each round and the player picks among them to decide what to do. For us, these experts are our strategies that will map a valuation to a bid. At each time step, the player picks the action to play and then after that, some adversary picks the utilities to assign for each action that could have been taken. This is a stronger condition then we will need to use as the for us, the adversary is the cumulative action of the other players, where the highest bid determines which strategies (if any) the player could have taken and won. However, in the general case this algorithm has been shown to be no-regret in the face of an adversary directly picking the utilities the learning agent receives.\\

\begin{algorithm}[H]
	Initialize $w^1(a) = 1$ for every $a\in A$\\
	\For{$t = 1, 2, \ldots, T$}{
		use ditribution $p^t = \frac{w^t}{\sum_{a \in A} w^t(a)}$ over actions to pick $a \in A$ and output $a$.\\
		Given the utility vector $u^t$, for every action $a \in A$ use the formula $w^{t+1}(a) = w^t(a) \cdot (1 - \eta u^t(a))$ to update its weight.\\
		\caption{Multiplicative Weights (MW) Algorithm}
	}
\end{algorithm}
\vspace{1cm}
The logic of this algorithm is rather simple, at each time step we see how well each of the possible actions performed and increase the weight, or probability of selecting that action in the future proportionally to how well it did as determined by the outside parameter $\eta$, the \textit{learning rate}. As demonstrated in (CITE Roughgarden lecture notes), the MW algorithm is no-regret if $\eta = \sqrt{(\ln n) / T}$ where $n$ is the number of actions that this agent can choose from, and $T$ is the number of rounds that will be played (yes this assumes that the player know that up front).

This algorithm fulfills our purpose of simulating agents learning in auctions, but in some sense it is unsatisfying that the learning algorithm requires a finite set of actions that the bidder does not even get to choose. It would be more interesting if we were able to give our agents some reasonable algorithm that allowed them to formulate their own strategies or mappings between their valuation and their bid. This introduces a whole host of other problems from the reasonableness of expect agents to implement such algorithms to the ability to prove that such algorithms converge to an equilibria. Part of the beauty of using multiplicative weights is that it is simple and had nice mathematical properties. In order to explore more interesting learning techniques both of these properties might be lost and then we would have to more and more wonder what our simulation is really showing\footnote{This thesis grew out of an interest in doing just that, throwing "smarter" algorithms such as neural networks and machine learning into existing multi-agent simulations. It becomes hard to tell what the point of such simulations are when the dynamics might just be properties of the interaction of the specific algorithms used and not of the system itself.}?

\subsection{Uniform Distribution Simulations}
The first simulation we run is a repeat of the symmetric auction setting, but now using agents learning with the multiplicative weights algorithm. We create 101 strategies that bidders can choose from to shade their bid, from bidding zero percent of their valuation with one percent increases up to bidding 100 percent of their valuation. that is $S = \{ 0, 0.01 \cdot v_i, 0.02 \cdot v_i, \ldots, 0.99 \cdot v_i, v_i \}$. First, we run the simulation with two symmetric bidders each choosing their valuation from the uniform distribution over $[0,1]$. The results are shown below in table \ref{table:3}

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c| }
			\hline
			Round & POA \\
			\hline
			1 & 1.0000 \\
			10 & 0.9517 \\
			1,000 & 0.9129 \\
			10,000 & 0.9578 \\
			100,000 & 0.9947 \\
			\hline
		\end{tabular}
		\caption{Price of anarchy in two player asymmetric auction with no-regret learning}
		\label{table:3}
	\end{center} 
\end{table}

Here we can see that after 100,000 rounds the price of anarchy converges to 0.99 and near perfect efficiency as the agents learn how to play the game. It's important to point out here that the above table is not an average, but simply one run of the simulation. Since each time we run the simulation, it is possible for the bidders to learn to converge to a new equilibrium the POA values for each simulation can be different. Averages don't make sense in this context as we are more concerned with the lowest possible POA that the system converges too. We now repeat this using 2,10 and 100, and 1,000 agents, each symmetric and drawing from a uniform $[0,1]$ as above. This time however, we only report the minimum POA of running 100 simulations with that number of agents \footnote{This is a relatively small number, but necessarily chosen for the sake of computation time with 1,000 agents}. This gives us the following results as shown  below in table \ref{table:4} (TODO: this table is incomplete, and isn't minimums):

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c|c|c| }
			\hline
			Round & 10 Agent POA & 100 Agent POA\\
			\hline
			1 & 0.9261 & (TODO, run on faster computer)\\
			10 & 0.9393 & \\
			1,000 & 0.9475 & \\
			10,000 & 0.9485 &\\
			100,000 & 0.9613 & \\
			\hline
		\end{tabular}
		\caption{Price of anarchy in two player asymmetric auction with no-regret learning}
		\label{table:4}
	\end{center} 
\end{table}

As we can see, the more agents we have,the closer (non-monotonically) the price of anarchy moves to 1.00 without having to learn. This is simply because the more agents we have, the more we expect someone with a high valuation to win the auction no matter what strategy each bidder is using.

Finally, we simulate the case of having two bidders with asymmetric valuation distributions where one draws uniformly from $[0,1]$ and the other draws from $[0,2]$. We can see the result of simulating this 100,000 times in table \ref{table:5} below.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c| }
			\hline
			Round & POA \\
			\hline
			1 & 1.0000 \\
			10 & 0.8901 \\
			1,000 & 0.9314 \\
			10,000 & 0.9770 \\
			100,000 & 0.9961 \\
			\hline
		\end{tabular}
		\caption{Price of anarchy in two player asymmetric auction with no-regret learning}
		\label{table:5}
	\end{center} 
\end{table}

Here we see the agents converge to a very good efficiency similar to the behavior we saw when using the Bayes-Nash equilibrium for this setting and better than in the two bidder minimum-intelligence case.


\chapter*{Conclusion}
         \addcontentsline{toc}{chapter}{Conclusion}
	\chaptermark{Conclusion}
	\markboth{Conclusion}{Conclusion}
	\setcounter{chapter}{4}
	\setcounter{section}{0}
	
Here's a conclusion, demonstrating the use of all that manual incrementing and table of contents adding that has to happen if you use the starred form of the chapter command. The deal is, the chapter command in \LaTeX\ does a lot of things: it increments the chapter counter, it resets the section counter to zero, it puts the name of the chapter into the table of contents and the running headers, and probably some other stuff. 

So, if you remove all that stuff because you don't like it to say ``Chapter 4: Conclusion'', then you have to manually add all the things \LaTeX\ would normally do for you. Maybe someday we'll write a new chapter macro that doesn't add ``Chapter X'' to the beginning of every chapter title.

\section{More info}
And here's some other random info: the first paragraph after a chapter title or section head \emph{shouldn't be} indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there. 


%If you feel it necessary to include an appendix, it goes here.
    \appendix
      \chapter{The First Appendix}
      \chapter{The Second Appendix, for Fun}


%This is where endnotes are supposed to go, if you have them.
%I have no idea how endnotes work with LaTeX.

  \backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
    \nocite{*}

% Rename my bibliography to be called "Works Cited" and not "References" or ``Bibliography''
% \renewcommand{\bibname}{Works Cited}

%    \bibliographystyle{bsts/mla-good} % there are a variety of styles available; 
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can refer to files in the bsts or APA 
% subfolder, e.g. 
 \bibliographystyle{APA/reedecon}  % or
 \bibliography{thesis}
 % Comment the above two lines and uncomment the next line to use biblatex-chicago.
 %\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\end{document}
